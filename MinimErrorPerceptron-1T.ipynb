{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_data(input_path, pattern):\n",
    "    with open(input_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    matches = re.findall(pattern, content, re.MULTILINE)\n",
    "    data = []\n",
    "    for match in matches:\n",
    "        numbers = match[1].replace('\\n', ' ').split()\n",
    "        numbers_float = [float(number) for number in numbers]\n",
    "        data.append(numbers_float)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "rocks_path = 'data/sonar.rocks'\n",
    "mines_path = 'data/sonar.mines'\n",
    "\n",
    "\n",
    "train_pattern = r'\\*(CR|CM)\\d+:\\n\\{([\\d\\s\\.\\n]+)\\}'\n",
    "test_pattern = r'^(?!\\*)(CR|CM)\\d+:\\n\\{([\\d\\s\\.\\n]+)\\}'\n",
    "\n",
    "\n",
    "rocks_train_df = extract_data(rocks_path, train_pattern)\n",
    "mines_train_df = extract_data(mines_path, train_pattern)\n",
    "rocks_train_df['Label'] = 'R'  \n",
    "mines_train_df['Label'] = 'M' \n",
    "train_df = pd.concat([rocks_train_df, mines_train_df], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "rocks_test_df = extract_data(rocks_path, test_pattern)\n",
    "mines_test_df = extract_data(mines_path, test_pattern)\n",
    "rocks_test_df['Label'] = 'R'  \n",
    "mines_test_df['Label'] = 'M' \n",
    "test_df = pd.concat([rocks_test_df, mines_test_df], ignore_index=True).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7258497   0.21159445  0.38491744  0.32606422  0.53436995  0.56897422\n",
      "  0.21526413  0.05270522 -0.25925507  0.44453582  0.5752605   0.82109167\n",
      "  0.86641213  0.82248482  0.28308209 -0.28716421 -0.91566584 -0.54622001\n",
      " -0.21480961  0.23341056  0.20456587  0.3123386   0.38649658  0.09747376\n",
      "  0.19790741 -0.14322522 -0.36219101 -0.1941907   0.04100954  0.25692631\n",
      "  0.18740009 -0.76457626  0.05496442 -0.05310656 -0.41353865 -0.67201051\n",
      " -0.86356772 -0.62081879  0.27156894  0.21043719 -0.51119612  0.111341\n",
      "  0.49329699  0.55662578  0.89377351  1.40883839  1.18875956  0.84899708\n",
      "  0.75775999  0.56093144  0.04599946  0.09881678  0.09159663  0.02209409\n",
      "  0.05353741 -0.00682674  0.00601123 -0.0147758   0.04354615  0.03551326\n",
      "  0.0123419 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def minimerror_perceptron(training_features, training_labels, epochs, learning_rate, T_initial, T_decrease_factor):\n",
    "    \n",
    "    weights = np.zeros(training_features.shape[1] + 1)\n",
    "    T = T_initial\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        \n",
    "        update = np.zeros(training_features.shape[1] + 1)\n",
    "        \n",
    "        for inputs, label in zip(training_features, training_labels):\n",
    "            \n",
    "            activation = np.dot(inputs, weights[1:]) + weights[0]\n",
    "            predicted_label = np.tanh(activation / T)\n",
    "            \n",
    "            # Mise à jour de la règle Minimerror, en utilisant la tangente hyperbolique pour la \"température\"\n",
    "            error = label - predicted_label\n",
    "            update[1:] += learning_rate * error * inputs\n",
    "            update[0] += learning_rate * error\n",
    "        \n",
    "        weights += update\n",
    "        \n",
    "        # Diminution de la \"température\" après chaque époque pour affiner l'apprentissage\n",
    "        T *= T_decrease_factor\n",
    "    \n",
    "    return weights\n",
    "\n",
    "train_df['Label'] = train_df['Label'].apply(lambda x: 1 if x == 'M' else -1)\n",
    "test_df['Label'] = test_df['Label'].apply(lambda x: 1 if x == 'M' else -1)\n",
    "\n",
    "\n",
    "train_features = train_df.iloc[:, :-1].values\n",
    "train_labels = train_df.iloc[:, -1].values\n",
    "weights = minimerror_perceptron(train_features, train_labels, 600, 0.001, T_initial=80, T_decrease_factor=0.99)\n",
    "print (weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ea et Eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur d'apprentissage (Ea) : 0.1923076923076923\n",
      "Erreur de généralisation (Eg) : 0.14423076923076927\n"
     ]
    }
   ],
   "source": [
    "def predict_perceptron(inputs, weights):\n",
    "    activation = np.dot(inputs, weights[1:]) + weights[0]\n",
    "    return 1 if activation >= 0 else -1\n",
    "\n",
    "\n",
    "def calculate_accuracy(features, labels, weights):\n",
    "    predictions = [predict_perceptron(x, weights) for x in features]\n",
    "    correct_predictions = sum(pred == label for pred, label in zip(predictions, labels))\n",
    "    accuracy = correct_predictions / len(labels)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "test_features = test_df.iloc[:, :-1].values\n",
    "test_labels = test_df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "training_accuracy = calculate_accuracy(train_features, train_labels, weights)\n",
    "testing_accuracy = calculate_accuracy(test_features, test_labels, weights)\n",
    "\n",
    "\n",
    "Ea = 1 - training_accuracy\n",
    "Eg = 1 - testing_accuracy\n",
    "\n",
    "print(f\"Erreur d'apprentissage (Ea) : {Ea}\")\n",
    "print(f\"Erreur de généralisation (Eg) : {Eg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def minimerror_perceptron2(training_features, training_labels, epochs, learning_rate, T_initial, T_decrease_factor):\n",
    "    # Initialisation des poids avec une petite valeur non nulle pour éviter la division par zéro.\n",
    "    weights = np.random.uniform(low=-0.5, high=0.5, size=training_features.shape[1] + 1)\n",
    "    T = T_initial  # Température initiale\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, label in zip(training_features, training_labels):\n",
    "            # Calcul de la stabilité pour l'exemple actuel\n",
    "            gamma = label * (np.dot(inputs, weights[1:]) + weights[0])\n",
    "            \n",
    "            # Calcul de la contribution à la fonction de coût pour cet exemple\n",
    "            V = 0.5 * (1 - np.tanh(gamma / (2 * T)))\n",
    "            \n",
    "            # Calcul de la dérivée de V par rapport à gamma\n",
    "            dV_dgamma = -0.5 * (1 - np.tanh(gamma / (2 * T))**2) * (1 / (2 * T))\n",
    "            \n",
    "            # Mise à jour des poids - la descente de gradient sur la fonction de coût\n",
    "            weights[1:] -= learning_rate * dV_dgamma * label * inputs\n",
    "            weights[0] -= learning_rate * dV_dgamma * label\n",
    "        \n",
    "        # Diminution de la température après chaque époque pour affiner l'apprentissage\n",
    "        T *= T_decrease_factor\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Paramètres de l'algorithme (à ajuster selon le problème)\n",
    "epochs = 1000\n",
    "learning_rate = 0.001\n",
    "T_initial = 100.0  # Température initiale élevée\n",
    "T_decrease_factor = 0.99  # Facteur de diminution de la température\n",
    "\n",
    "weights2 = minimerror_perceptron2(train_features, train_labels, epochs, learning_rate, T_initial, T_decrease_factor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ea et Eg 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur d'apprentissage (Ea) : 0.17307692307692313\n",
      "Erreur de généralisation (Eg) : 0.22115384615384615\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_accuracy2 = calculate_accuracy(train_features, train_labels, weights2)\n",
    "testing_accuracy2 = calculate_accuracy(test_features, test_labels, weights2)\n",
    "\n",
    "\n",
    "Ea2 = 1 - training_accuracy2\n",
    "Eg2 = 1 - testing_accuracy2\n",
    "\n",
    "print(f\"Erreur d'apprentissage (Ea) : {Ea2}\")\n",
    "print(f\"Erreur de généralisation (Eg) : {Eg2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000, Erreurs: 55, T+: 99.0, T-: 0.99, Learning Rate: 0.02\n",
      "Epoch 2/10000, Erreurs: 55, T+: 100.0, T-: 1.0, Learning Rate: 0.019999980000020002\n",
      "Epoch 3/10000, Erreurs: 55, T+: 101.01010101010101, T-: 1.0101010101010102, Learning Rate: 0.01999994000014\n",
      "Epoch 4/10000, Erreurs: 55, T+: 102.03040506070809, T-: 1.020304050607081, Learning Rate: 0.0199998800005\n",
      "Epoch 5/10000, Erreurs: 55, T+: 103.06101521283645, T-: 1.0306101521283648, Learning Rate: 0.0199998000013\n",
      "Epoch 6/10000, Erreurs: 55, T+: 104.10203556852167, T-: 1.041020355685217, Learning Rate: 0.019999700002799985\n",
      "Epoch 7/10000, Erreurs: 55, T+: 105.15357128133502, T-: 1.0515357128133505, Learning Rate: 0.019999580005319952\n",
      "Epoch 8/10000, Erreurs: 55, T+: 106.21572856700507, T-: 1.062157285670051, Learning Rate: 0.019999440009239887\n",
      "Epoch 9/10000, Erreurs: 44, T+: 105.15357128133502, T-: 1.0515357128133505, Learning Rate: 0.019999280014999767\n",
      "Epoch 10/10000, Erreurs: 41, T+: 104.10203556852167, T-: 1.041020355685217, Learning Rate: 0.019999100023099563\n",
      "Epoch 11/10000, Erreurs: 39, T+: 103.06101521283645, T-: 1.0306101521283648, Learning Rate: 0.01999890003409922\n",
      "Epoch 12/10000, Erreurs: 39, T+: 104.10203556852167, T-: 1.041020355685217, Learning Rate: 0.019998680048618687\n",
      "Epoch 13/10000, Erreurs: 40, T+: 105.15357128133502, T-: 1.0515357128133505, Learning Rate: 0.01999844006733788\n",
      "Epoch 14/10000, Erreurs: 41, T+: 106.21572856700507, T-: 1.062157285670051, Learning Rate: 0.019998180090996697\n",
      "Epoch 15/10000, Erreurs: 41, T+: 107.28861471414653, T-: 1.0728861471414657, Learning Rate: 0.019997900120395012\n",
      "Epoch 16/10000, Erreurs: 40, T+: 108.3723380950975, T-: 1.0837233809509754, Learning Rate: 0.019997600156392664\n",
      "Epoch 17/10000, Erreurs: 39, T+: 109.46700817686617, T-: 1.094670081768662, Learning Rate: 0.019997280199909463\n",
      "Epoch 18/10000, Erreurs: 35, T+: 108.3723380950975, T-: 1.0837233809509754, Learning Rate: 0.01999694025192518\n",
      "Epoch 19/10000, Erreurs: 35, T+: 109.46700817686617, T-: 1.094670081768662, Learning Rate: 0.019996580313479537\n",
      "Epoch 20/10000, Erreurs: 31, T+: 108.3723380950975, T-: 1.0837233809509754, Learning Rate: 0.019996200385672208\n",
      "Epoch 21/10000, Erreurs: 30, T+: 107.28861471414653, T-: 1.0728861471414657, Learning Rate: 0.019995800469662817\n",
      "Epoch 22/10000, Erreurs: 27, T+: 106.21572856700507, T-: 1.062157285670051, Learning Rate: 0.019995380566670915\n",
      "Epoch 23/10000, Erreurs: 29, T+: 107.28861471414653, T-: 1.0728861471414657, Learning Rate: 0.019994940677976\n",
      "Epoch 24/10000, Erreurs: 29, T+: 108.3723380950975, T-: 1.0837233809509754, Learning Rate: 0.019994480804917485\n",
      "Epoch 25/10000, Erreurs: 29, T+: 109.46700817686617, T-: 1.094670081768662, Learning Rate: 0.01999400094889471\n",
      "Epoch 26/10000, Erreurs: 28, T+: 110.57273553218805, T-: 1.105727355321881, Learning Rate: 0.019993501111366926\n",
      "Epoch 27/10000, Erreurs: 26, T+: 109.46700817686617, T-: 1.094670081768662, Learning Rate: 0.019992981293853285\n",
      "Epoch 28/10000, Erreurs: 26, T+: 110.57273553218805, T-: 1.105727355321881, Learning Rate: 0.01999244149793284\n",
      "Epoch 29/10000, Erreurs: 28, T+: 111.689631850695, T-: 1.1168963185069505, Learning Rate: 0.019991881725244534\n",
      "Epoch 30/10000, Erreurs: 26, T+: 112.81780995019696, T-: 1.1281780995019701, Learning Rate: 0.019991301977487184\n",
      "Epoch 31/10000, Erreurs: 25, T+: 111.689631850695, T-: 1.1168963185069505, Learning Rate: 0.019990702256419492\n",
      "Epoch 32/10000, Erreurs: 24, T+: 110.57273553218805, T-: 1.105727355321881, Learning Rate: 0.019990082563860014\n",
      "Epoch 33/10000, Erreurs: 23, T+: 109.46700817686617, T-: 1.094670081768662, Learning Rate: 0.01998944290168716\n",
      "Epoch 34/10000, Erreurs: 23, T+: 110.57273553218805, T-: 1.105727355321881, Learning Rate: 0.01998878327183919\n",
      "Epoch 35/10000, Erreurs: 23, T+: 111.689631850695, T-: 1.1168963185069505, Learning Rate: 0.019988103676314192\n",
      "Epoch 36/10000, Erreurs: 22, T+: 110.57273553218805, T-: 1.105727355321881, Learning Rate: 0.01998740411717009\n",
      "Epoch 37/10000, Erreurs: 22, T+: 111.689631850695, T-: 1.1168963185069505, Learning Rate: 0.019986684596524616\n",
      "Epoch 38/10000, Erreurs: 23, T+: 112.81780995019696, T-: 1.1281780995019701, Learning Rate: 0.0199859451165553\n",
      "Epoch 39/10000, Erreurs: 23, T+: 113.95738378807773, T-: 1.1395738378807778, Learning Rate: 0.01998518567949948\n",
      "Epoch 40/10000, Erreurs: 23, T+: 115.1084684728058, T-: 1.1510846847280585, Learning Rate: 0.019984406287654265\n",
      "Epoch 41/10000, Erreurs: 23, T+: 116.2711802755614, T-: 1.1627118027556147, Learning Rate: 0.019983606943376527\n",
      "Epoch 42/10000, Erreurs: 19, T+: 115.1084684728058, T-: 1.1510846847280585, Learning Rate: 0.019982787649082914\n",
      "Epoch 43/10000, Erreurs: 19, T+: 116.2711802755614, T-: 1.1627118027556147, Learning Rate: 0.019981948407249806\n",
      "Epoch 44/10000, Erreurs: 19, T+: 117.44563664198122, T-: 1.1744563664198129, Learning Rate: 0.019981089220413327\n",
      "Epoch 45/10000, Erreurs: 20, T+: 118.63195620402144, T-: 1.186319562040215, Learning Rate: 0.019980210091169317\n",
      "Epoch 46/10000, Erreurs: 19, T+: 119.83025879194085, T-: 1.1983025879194091, Learning Rate: 0.01997931102217332\n",
      "Epoch 47/10000, Erreurs: 20, T+: 121.0406654464049, T-: 1.2104066544640497, Learning Rate: 0.019978392016140576\n",
      "Epoch 48/10000, Erreurs: 22, T+: 122.26329843071203, T-: 1.2226329843071209, Learning Rate: 0.019977453075846013\n",
      "Epoch 49/10000, Erreurs: 22, T+: 123.49828124314347, T-: 1.2349828124314353, Learning Rate: 0.019976494204124215\n",
      "Epoch 50/10000, Erreurs: 21, T+: 124.74573862943785, T-: 1.247457386294379, Learning Rate: 0.019975515403869427\n",
      "Epoch 51/10000, Erreurs: 18, T+: 123.49828124314347, T-: 1.2349828124314353, Learning Rate: 0.01997451667803552\n",
      "Epoch 52/10000, Erreurs: 18, T+: 124.74573862943785, T-: 1.247457386294379, Learning Rate: 0.01997349802963601\n",
      "Epoch 53/10000, Erreurs: 18, T+: 126.00579659539177, T-: 1.2600579659539182, Learning Rate: 0.019972459461744002\n",
      "Epoch 54/10000, Erreurs: 21, T+: 127.27858241958765, T-: 1.272785824195877, Learning Rate: 0.019971400977492195\n",
      "Epoch 55/10000, Erreurs: 22, T+: 128.56422466625014, T-: 1.285642246662502, Learning Rate: 0.01997032258007287\n",
      "Epoch 56/10000, Erreurs: 18, T+: 129.86285319823247, T-: 1.2986285319823252, Learning Rate: 0.01996922427273787\n",
      "Epoch 57/10000, Erreurs: 18, T+: 131.17459919013382, T-: 1.3117459919013386, Learning Rate: 0.019968106058798576\n",
      "Epoch 58/10000, Erreurs: 22, T+: 132.49959514154932, T-: 1.3249959514154934, Learning Rate: 0.019966967941625904\n",
      "Epoch 59/10000, Erreurs: 18, T+: 133.83797489045386, T-: 1.3383797489045388, Learning Rate: 0.019965809924650278\n",
      "Epoch 60/10000, Erreurs: 21, T+: 135.18987362672107, T-: 1.351898736267211, Learning Rate: 0.019964632011361606\n",
      "Epoch 61/10000, Erreurs: 21, T+: 136.55542790577886, T-: 1.3655542790577888, Learning Rate: 0.01996343420530929\n",
      "Epoch 62/10000, Erreurs: 18, T+: 137.9347756624029, T-: 1.379347756624029, Learning Rate: 0.01996221651010217\n",
      "Epoch 63/10000, Erreurs: 19, T+: 139.3280562246494, T-: 1.393280562246494, Learning Rate: 0.019960978929408547\n",
      "Epoch 64/10000, Erreurs: 21, T+: 140.7354103279287, T-: 1.407354103279287, Learning Rate: 0.01995972146695613\n",
      "Epoch 65/10000, Erreurs: 17, T+: 139.3280562246494, T-: 1.393280562246494, Learning Rate: 0.01995844412653203\n",
      "Epoch 66/10000, Erreurs: 19, T+: 140.7354103279287, T-: 1.407354103279287, Learning Rate: 0.01995714691198275\n",
      "Epoch 67/10000, Erreurs: 20, T+: 142.1569801292209, T-: 1.421569801292209, Learning Rate: 0.01995582982721416\n",
      "Epoch 68/10000, Erreurs: 22, T+: 143.59290922143526, T-: 1.4359290922143524, Learning Rate: 0.019954492876191453\n",
      "Epoch 69/10000, Erreurs: 18, T+: 145.04334264791441, T-: 1.4504334264791439, Learning Rate: 0.019953136062939174\n",
      "Epoch 70/10000, Erreurs: 18, T+: 146.50842691708527, T-: 1.4650842691708523, Learning Rate: 0.019951759391541157\n",
      "Epoch 71/10000, Erreurs: 18, T+: 147.98831001725785, T-: 1.479883100172578, Learning Rate: 0.019950362866140527\n",
      "Epoch 72/10000, Erreurs: 19, T+: 149.48314143157359, T-: 1.4948314143157355, Learning Rate: 0.019948946490939673\n",
      "Epoch 73/10000, Erreurs: 20, T+: 150.99307215310463, T-: 1.509930721531046, Learning Rate: 0.019947510270200218\n",
      "Epoch 74/10000, Erreurs: 18, T+: 152.5182547001057, T-: 1.5251825470010565, Learning Rate: 0.019946054208243018\n",
      "Epoch 75/10000, Erreurs: 18, T+: 154.0588431314199, T-: 1.5405884313141984, Learning Rate: 0.01994457830944812\n",
      "Epoch 76/10000, Erreurs: 18, T+: 155.6149930620403, T-: 1.5561499306204025, Learning Rate: 0.01994308257825475\n",
      "Epoch 77/10000, Erreurs: 18, T+: 157.1868616788286, T-: 1.5718686167882854, Learning Rate: 0.019941567019161294\n",
      "Epoch 78/10000, Erreurs: 18, T+: 158.77460775639253, T-: 1.5877460775639247, Learning Rate: 0.019940031636725267\n",
      "Epoch 79/10000, Erreurs: 20, T+: 160.37839167312376, T-: 1.603783916731237, Learning Rate: 0.01993847643556329\n",
      "Epoch 80/10000, Erreurs: 22, T+: 161.99837542739775, T-: 1.6199837542739768, Learning Rate: 0.019936901420351084\n",
      "Epoch 81/10000, Erreurs: 18, T+: 163.63472265393713, T-: 1.6363472265393706, Learning Rate: 0.019935306595823415\n",
      "Epoch 82/10000, Erreurs: 19, T+: 165.28759864034055, T-: 1.6528759864034046, Learning Rate: 0.019933691966774105\n",
      "Epoch 83/10000, Erreurs: 18, T+: 166.95717034377833, T-: 1.6695717034377824, Learning Rate: 0.019932057538055985\n",
      "Epoch 84/10000, Erreurs: 19, T+: 168.6436064078569, T-: 1.686436064078568, Learning Rate: 0.019930403314580875\n",
      "Epoch 85/10000, Erreurs: 21, T+: 170.34707717965344, T-: 1.7034707717965334, Learning Rate: 0.019928729301319566\n",
      "Epoch 86/10000, Erreurs: 22, T+: 172.06775472692266, T-: 1.7206775472692257, Learning Rate: 0.01992703550330179\n",
      "Epoch 87/10000, Erreurs: 18, T+: 173.80581285547743, T-: 1.7380581285547734, Learning Rate: 0.019925321925616184\n",
      "Epoch 88/10000, Erreurs: 18, T+: 175.5614271267449, T-: 1.7556142712674478, Learning Rate: 0.019923588573410297\n",
      "Epoch 89/10000, Erreurs: 17, T+: 177.3347748754999, T-: 1.7733477487549978, Learning Rate: 0.019921835451890527\n",
      "Epoch 90/10000, Erreurs: 20, T+: 179.1260352277777, T-: 1.7912603522777755, Learning Rate: 0.019920062566322123\n",
      "Epoch 91/10000, Erreurs: 21, T+: 180.93538911896738, T-: 1.8093538911896723, Learning Rate: 0.019918269922029143\n",
      "Epoch 92/10000, Erreurs: 21, T+: 182.76301931208826, T-: 1.8276301931208812, Learning Rate: 0.019916457524394422\n",
      "Epoch 93/10000, Erreurs: 20, T+: 184.60911041625076, T-: 1.8460911041625063, Learning Rate: 0.019914625378859567\n",
      "Epoch 94/10000, Erreurs: 18, T+: 186.4738489053038, T-: 1.8647384890530367, Learning Rate: 0.019912773490924913\n",
      "Epoch 95/10000, Erreurs: 20, T+: 188.35742313667052, T-: 1.8835742313667039, Learning Rate: 0.019910901866149494\n",
      "Epoch 96/10000, Erreurs: 20, T+: 190.26002337037426, T-: 1.9026002337037413, Learning Rate: 0.01990901051015103\n",
      "Epoch 97/10000, Erreurs: 21, T+: 192.18184178825683, T-: 1.921818417882567, Learning Rate: 0.01990709942860588\n",
      "Epoch 98/10000, Erreurs: 19, T+: 194.12307251339075, T-: 1.941230725133906, Learning Rate: 0.01990516862724904\n",
      "Epoch 99/10000, Erreurs: 19, T+: 196.08391162968763, T-: 1.9608391162968748, Learning Rate: 0.019903218111874076\n",
      "Epoch 100/10000, Erreurs: 19, T+: 198.06455720170467, T-: 1.9806455720170453, Learning Rate: 0.01990124788833313\n",
      "Epoch 101/10000, Erreurs: 20, T+: 200.0652092946512, T-: 2.0006520929465106, Learning Rate: 0.019899257962536877\n",
      "Epoch 102/10000, Erreurs: 19, T+: 202.08606999459718, T-: 2.0208606999459704, Learning Rate: 0.01989724834045449\n",
      "Epoch 103/10000, Erreurs: 19, T+: 204.12734342888604, T-: 2.041273434288859, Learning Rate: 0.01989521902811362\n",
      "Epoch 104/10000, Erreurs: 19, T+: 206.18923578675358, T-: 2.0618923578675346, Learning Rate: 0.019893170031600366\n",
      "Epoch 105/10000, Erreurs: 20, T+: 208.27195534015513, T-: 2.08271955340155, Learning Rate: 0.01989110135705923\n",
      "Epoch 106/10000, Erreurs: 20, T+: 210.37571246480317, T-: 2.1037571246480304, Learning Rate: 0.019889013010693107\n",
      "Epoch 107/10000, Erreurs: 18, T+: 212.50071966141735, T-: 2.125007196614172, Learning Rate: 0.01988690499876324\n",
      "Epoch 108/10000, Erreurs: 19, T+: 214.64719157718923, T-: 2.1464719157718912, Learning Rate: 0.019884777327589184\n",
      "Epoch 109/10000, Erreurs: 18, T+: 216.81534502746388, T-: 2.168153450274638, Learning Rate: 0.019882630003548802\n",
      "Epoch 110/10000, Erreurs: 20, T+: 219.0053990176403, T-: 2.190053990176402, Learning Rate: 0.019880463033078197\n",
      "Epoch 111/10000, Erreurs: 20, T+: 221.21757476529322, T-: 2.2121757476529313, Learning Rate: 0.019878276422671703\n",
      "Epoch 112/10000, Erreurs: 19, T+: 223.4520957225184, T-: 2.2345209572251834, Learning Rate: 0.01987607017888185\n",
      "Epoch 113/10000, Erreurs: 20, T+: 225.70918759850343, T-: 2.257091875985034, Learning Rate: 0.01987384430831932\n",
      "Epoch 114/10000, Erreurs: 19, T+: 227.9890783823267, T-: 2.2798907838232667, Learning Rate: 0.019871598817652925\n",
      "Epoch 115/10000, Erreurs: 21, T+: 230.2919983659866, T-: 2.3029199836598653, Learning Rate: 0.019869333713609574\n",
      "Epoch 116/10000, Erreurs: 19, T+: 232.6181801676632, T-: 2.326181801676632, Learning Rate: 0.01986704900297423\n",
      "Epoch 117/10000, Erreurs: 21, T+: 234.96785875521536, T-: 2.3496785875521535, Learning Rate: 0.019864744692589892\n",
      "Epoch 118/10000, Erreurs: 21, T+: 237.34127146991452, T-: 2.373412714699145, Learning Rate: 0.019862420789357536\n",
      "Epoch 119/10000, Erreurs: 21, T+: 239.7386580504187, T-: 2.3973865805041865, Learning Rate: 0.019860077300236107\n",
      "Epoch 120/10000, Erreurs: 21, T+: 242.16026065698858, T-: 2.4216026065698855, Learning Rate: 0.01985771423224247\n",
      "Epoch 121/10000, Erreurs: 20, T+: 244.60632389594807, T-: 2.4460632389594803, Learning Rate: 0.01985533159245138\n",
      "Epoch 122/10000, Erreurs: 20, T+: 247.07709484439198, T-: 2.4707709484439193, Learning Rate: 0.01985292938799543\n",
      "Epoch 123/10000, Erreurs: 20, T+: 249.57282307514342, T-: 2.4957282307514337, Learning Rate: 0.01985050762606505\n",
      "Epoch 124/10000, Erreurs: 21, T+: 252.09376068196306, T-: 2.52093760681963, Learning Rate: 0.01984806631390844\n",
      "Epoch 125/10000, Erreurs: 20, T+: 254.6401623050132, T-: 2.5464016230501314, Learning Rate: 0.019845605458831544\n",
      "Epoch 126/10000, Erreurs: 19, T+: 257.212285156579, T-: 2.5721228515657892, Learning Rate: 0.01984312506819802\n",
      "Epoch 127/10000, Erreurs: 19, T+: 259.8103890470495, T-: 2.5981038904704943, Learning Rate: 0.019840625149429192\n",
      "Epoch 128/10000, Erreurs: 21, T+: 262.4347364111611, T-: 2.6243473641116104, Learning Rate: 0.019838105710004023\n",
      "Epoch 129/10000, Erreurs: 22, T+: 265.0855923345062, T-: 2.650855923345061, Learning Rate: 0.01983556675745907\n",
      "Epoch 130/10000, Erreurs: 22, T+: 267.76322458030927, T-: 2.6776322458030917, Learning Rate: 0.019833008299388447\n",
      "Epoch 131/10000, Erreurs: 22, T+: 270.467903616474, T-: 2.7046790361647393, Learning Rate: 0.0198304303434438\n",
      "Epoch 132/10000, Erreurs: 21, T+: 273.19990264290306, T-: 2.7319990264290297, Learning Rate: 0.01982783289733425\n",
      "Epoch 133/10000, Erreurs: 21, T+: 275.959497619094, T-: 2.759594976190939, Learning Rate: 0.019825215968826363\n",
      "Epoch 134/10000, Erreurs: 22, T+: 278.7469672920142, T-: 2.7874696729201407, Learning Rate: 0.01982257956574412\n",
      "Epoch 135/10000, Erreurs: 22, T+: 281.5625932242568, T-: 2.8156259322425665, Learning Rate: 0.01981992369596886\n",
      "Epoch 136/10000, Erreurs: 22, T+: 284.4066598224816, T-: 2.844066598224815, Learning Rate: 0.019817248367439255\n",
      "Epoch 137/10000, Erreurs: 21, T+: 287.279454366143, T-: 2.872794543661429, Learning Rate: 0.01981455358815127\n",
      "Epoch 138/10000, Erreurs: 21, T+: 290.1812670365081, T-: 2.90181267036508, Learning Rate: 0.019811839366158105\n",
      "Epoch 139/10000, Erreurs: 20, T+: 293.1123909459678, T-: 2.931123909459677, Learning Rate: 0.019809105709570184\n",
      "Epoch 140/10000, Erreurs: 20, T+: 296.0731221676442, T-: 2.960731221676441, Learning Rate: 0.019806352626555095\n",
      "Epoch 141/10000, Erreurs: 20, T+: 299.06375976529716, T-: 2.990637597652971, Learning Rate: 0.019803580125337547\n",
      "Epoch 142/10000, Erreurs: 20, T+: 302.0846058235325, T-: 3.0208460582353243, Learning Rate: 0.019800788214199345\n",
      "Epoch 143/10000, Erreurs: 20, T+: 305.13596547831565, T-: 3.051359654783156, Learning Rate: 0.019797976901479334\n",
      "Epoch 144/10000, Erreurs: 20, T+: 308.21814694779357, T-: 3.082181469477935, Learning Rate: 0.019795146195573366\n",
      "Epoch 145/10000, Erreurs: 21, T+: 311.33146156342787, T-: 3.1133146156342777, Learning Rate: 0.019792296104934257\n",
      "Epoch 146/10000, Erreurs: 21, T+: 314.4762238014423, T-: 3.144762238014422, Learning Rate: 0.019789426638071737\n",
      "Epoch 147/10000, Erreurs: 21, T+: 317.65275131458816, T-: 3.176527513145881, Learning Rate: 0.01978653780355242\n",
      "Epoch 148/10000, Erreurs: 19, T+: 320.86136496423046, T-: 3.208613649642304, Learning Rate: 0.01978362960999975\n",
      "Epoch 149/10000, Erreurs: 21, T+: 324.10238885275805, T-: 3.2410238885275797, Learning Rate: 0.019780702066093964\n",
      "Epoch 150/10000, Erreurs: 20, T+: 327.37615035632126, T-: 3.273761503563212, Learning Rate: 0.019777755180572062\n",
      "Epoch 151/10000, Erreurs: 19, T+: 330.68298015790026, T-: 3.306829801579002, Learning Rate: 0.019774788962227726\n",
      "Epoch 152/10000, Erreurs: 21, T+: 334.02321228070736, T-: 3.340232122807073, Learning Rate: 0.01977180341991132\n",
      "Epoch 153/10000, Erreurs: 21, T+: 337.3971841219266, T-: 3.3739718412192654, Learning Rate: 0.019768798562529815\n",
      "Epoch 154/10000, Erreurs: 21, T+: 340.80523648679457, T-: 3.408052364867945, Learning Rate: 0.019765774399046757\n",
      "Epoch 155/10000, Erreurs: 21, T+: 344.2477136230248, T-: 3.4424771362302473, Learning Rate: 0.019762730938482232\n",
      "Epoch 156/10000, Erreurs: 17, T+: 347.72496325558063, T-: 3.4772496325558055, Learning Rate: 0.0197596681899128\n",
      "Epoch 157/10000, Erreurs: 18, T+: 351.23733662179865, T-: 3.5123733662179855, Learning Rate: 0.019756586162471453\n",
      "Epoch 158/10000, Erreurs: 21, T+: 354.7851885068673, T-: 3.547851885068672, Learning Rate: 0.019753484865347594\n",
      "Epoch 159/10000, Erreurs: 20, T+: 358.36887727966393, T-: 3.5836887727966387, Learning Rate: 0.019750364307786962\n",
      "Epoch 160/10000, Erreurs: 20, T+: 361.98876492895346, T-: 3.619887649289534, Learning Rate: 0.019747224499091606\n",
      "Epoch 161/10000, Erreurs: 20, T+: 365.645217099953, T-: 3.6564521709995295, Learning Rate: 0.019744065448619827\n",
      "Epoch 162/10000, Erreurs: 20, T+: 369.3386031312657, T-: 3.693386031312656, Learning Rate: 0.019740887165786133\n",
      "Epoch 163/10000, Erreurs: 20, T+: 373.0692960921876, T-: 3.730692960921875, Learning Rate: 0.019737689660061203\n",
      "Epoch 164/10000, Erreurs: 20, T+: 376.8376728203915, T-: 3.768376728203914, Learning Rate: 0.019734472940971824\n",
      "Epoch 165/10000, Erreurs: 20, T+: 380.64411395999144, T-: 3.8064411395999134, Learning Rate: 0.019731237018100853\n",
      "Epoch 166/10000, Erreurs: 20, T+: 384.4890039999914, T-: 3.8448900399999126, Learning Rate: 0.019727981901087176\n",
      "Epoch 167/10000, Erreurs: 19, T+: 388.3727313131226, T-: 3.883727313131225, Learning Rate: 0.019724707599625635\n",
      "Epoch 168/10000, Erreurs: 21, T+: 392.2956881950733, T-: 3.922956881950732, Learning Rate: 0.019721414123467014\n",
      "Epoch 169/10000, Erreurs: 20, T+: 396.25827090411445, T-: 3.9625827090411434, Learning Rate: 0.01971810148241797\n",
      "Epoch 170/10000, Erreurs: 20, T+: 400.2608797011257, T-: 4.002608797011256, Learning Rate: 0.019714769686340974\n",
      "Epoch 171/10000, Erreurs: 20, T+: 404.30391889002595, T-: 4.043039188900258, Learning Rate: 0.019711418745154297\n",
      "Epoch 172/10000, Erreurs: 19, T+: 408.3877968586121, T-: 4.08387796858612, Learning Rate: 0.01970804866883193\n",
      "Epoch 173/10000, Erreurs: 21, T+: 412.5129261198102, T-: 4.125129261198101, Learning Rate: 0.019704659467403535\n",
      "Epoch 174/10000, Erreurs: 20, T+: 416.6797233533436, T-: 4.166797233533435, Learning Rate: 0.019701251150954422\n",
      "Epoch 175/10000, Erreurs: 20, T+: 420.8886094478218, T-: 4.208886094478217, Learning Rate: 0.01969782372962547\n",
      "Epoch 176/10000, Erreurs: 19, T+: 425.14000954325434, T-: 4.251400095432542, Learning Rate: 0.019694377213613088\n",
      "Epoch 177/10000, Erreurs: 19, T+: 429.43435307399426, T-: 4.294343530739941, Learning Rate: 0.019690911613169174\n",
      "Epoch 178/10000, Erreurs: 19, T+: 433.7720738121154, T-: 4.337720738121153, Learning Rate: 0.019687426938601038\n",
      "Epoch 179/10000, Erreurs: 19, T+: 438.1536099112277, T-: 4.381536099112275, Learning Rate: 0.01968392320027139\n",
      "Epoch 180/10000, Erreurs: 19, T+: 442.57940395073507, T-: 4.425794039507349, Learning Rate: 0.01968040040859825\n",
      "Epoch 181/10000, Erreurs: 19, T+: 447.0499029805405, T-: 4.470499029805403, Learning Rate: 0.019676858574054917\n",
      "Epoch 182/10000, Erreurs: 19, T+: 451.5655585662025, T-: 4.5156555856620235, Learning Rate: 0.01967329770716992\n",
      "Epoch 183/10000, Erreurs: 19, T+: 456.126826834548, T-: 4.561268268345478, Learning Rate: 0.019669717818526947\n",
      "Epoch 184/10000, Erreurs: 18, T+: 460.7341685197455, T-: 4.607341685197453, Learning Rate: 0.01966611891876481\n",
      "Epoch 185/10000, Erreurs: 18, T+: 465.38804900984394, T-: 4.653880490098437, Learning Rate: 0.019662501018577392\n",
      "Epoch 186/10000, Erreurs: 18, T+: 470.08893839378175, T-: 4.700889383937816, Learning Rate: 0.01965886412871358\n",
      "Epoch 187/10000, Erreurs: 18, T+: 474.83731150887047, T-: 4.748373115088703, Learning Rate: 0.019655208259977224\n",
      "Epoch 188/10000, Erreurs: 19, T+: 479.63364798875807, T-: 4.796336479887579, Learning Rate: 0.01965153342322708\n",
      "Epoch 189/10000, Erreurs: 19, T+: 484.4784323118768, T-: 4.844784323118766, Learning Rate: 0.019647839629376756\n",
      "Epoch 190/10000, Erreurs: 19, T+: 489.37215385038064, T-: 4.893721538503804, Learning Rate: 0.01964412688939466\n",
      "Epoch 191/10000, Erreurs: 20, T+: 494.3153069195764, T-: 4.943153069195762, Learning Rate: 0.019640395214303944\n",
      "Epoch 192/10000, Erreurs: 20, T+: 499.30839082785496, T-: 4.993083908278547, Learning Rate: 0.019636644615182445\n",
      "Epoch 193/10000, Erreurs: 18, T+: 504.35190992712626, T-: 5.0435190992712595, Learning Rate: 0.01963287510316264\n",
      "Epoch 194/10000, Erreurs: 17, T+: 509.4463736637639, T-: 5.0944637366376355, Learning Rate: 0.019629086689431575\n",
      "Epoch 195/10000, Erreurs: 18, T+: 514.5922966300645, T-: 5.145922966300642, Learning Rate: 0.01962527938523084\n",
      "Epoch 196/10000, Erreurs: 19, T+: 519.7901986162268, T-: 5.197901986162265, Learning Rate: 0.019621453201856476\n",
      "Epoch 197/10000, Erreurs: 20, T+: 525.0406046628553, T-: 5.25040604662855, Learning Rate: 0.019617608150658945\n",
      "Epoch 198/10000, Erreurs: 19, T+: 530.3440451139953, T-: 5.3034404511399496, Learning Rate: 0.019613744243043067\n",
      "Epoch 199/10000, Erreurs: 20, T+: 535.7010556707023, T-: 5.35701055670702, Learning Rate: 0.019609861490467957\n",
      "Epoch 200/10000, Erreurs: 18, T+: 541.1121774451539, T-: 5.411121774451535, Learning Rate: 0.01960595990444697\n",
      "Epoch 201/10000, Erreurs: 18, T+: 546.5779570153069, T-: 5.4657795701530665, Learning Rate: 0.019602039496547662\n",
      "Epoch 202/10000, Erreurs: 19, T+: 552.098946480108, T-: 5.520989464801077, Learning Rate: 0.01959810027839171\n",
      "Epoch 203/10000, Erreurs: 20, T+: 557.6757035152606, T-: 5.576757035152603, Learning Rate: 0.019594142261654852\n",
      "Epoch 204/10000, Erreurs: 19, T+: 563.3087914295561, T-: 5.633087914295559, Learning Rate: 0.019590165458066865\n",
      "Epoch 205/10000, Erreurs: 19, T+: 568.9987792217738, T-: 5.689987792217736, Learning Rate: 0.019586169879411464\n",
      "Epoch 206/10000, Erreurs: 19, T+: 574.7462416381554, T-: 5.747462416381552, Learning Rate: 0.01958215553752627\n",
      "Epoch 207/10000, Erreurs: 18, T+: 580.55175923046, T-: 5.805517592304598, Learning Rate: 0.019578122444302747\n",
      "Epoch 208/10000, Erreurs: 18, T+: 586.415918414606, T-: 5.864159184146059, Learning Rate: 0.019574070611686128\n",
      "Epoch 209/10000, Erreurs: 19, T+: 592.3393115299051, T-: 5.923393115299049, Learning Rate: 0.01957000005167538\n",
      "Epoch 210/10000, Erreurs: 19, T+: 598.3225368988941, T-: 5.983225368988939, Learning Rate: 0.01956591077632313\n",
      "Epoch 211/10000, Erreurs: 20, T+: 604.3661988877718, T-: 6.043661988877716, Learning Rate: 0.019561802797735603\n",
      "Epoch 212/10000, Erreurs: 19, T+: 610.4709079674462, T-: 6.1047090796744605, Learning Rate: 0.01955767612807258\n",
      "Epoch 213/10000, Erreurs: 19, T+: 616.6372807751982, T-: 6.166372807751981, Learning Rate: 0.019553530779547315\n",
      "Epoch 214/10000, Erreurs: 17, T+: 622.8659401769679, T-: 6.2286594017696775, Learning Rate: 0.019549366764426493\n",
      "Epoch 215/10000, Erreurs: 19, T+: 629.1575153302706, T-: 6.291575153302705, Learning Rate: 0.019545184095030158\n",
      "Epoch 216/10000, Erreurs: 19, T+: 635.512641747748, T-: 6.3551264174774795, Learning Rate: 0.019540982783731654\n",
      "Epoch 217/10000, Erreurs: 20, T+: 641.9319613613617, T-: 6.419319613613616, Learning Rate: 0.019536762842957574\n",
      "Epoch 218/10000, Erreurs: 19, T+: 648.416122587234, T-: 6.484161225872339, Learning Rate: 0.01953252428518769\n",
      "Epoch 219/10000, Erreurs: 19, T+: 654.9657803911455, T-: 6.549657803911454, Learning Rate: 0.019528267122954884\n",
      "Epoch 220/10000, Erreurs: 18, T+: 661.5815963546924, T-: 6.615815963546923, Learning Rate: 0.019523991368845108\n",
      "Epoch 221/10000, Erreurs: 17, T+: 668.2642387421135, T-: 6.682642387421134, Learning Rate: 0.019519697035497295\n",
      "Epoch 222/10000, Erreurs: 19, T+: 675.0143825677915, T-: 6.750143825677913, Learning Rate: 0.019515384135603327\n",
      "Epoch 223/10000, Erreurs: 19, T+: 681.8327096644358, T-: 6.8183270966443565, Learning Rate: 0.019511052681907944\n",
      "Epoch 224/10000, Erreurs: 20, T+: 688.7199087519554, T-: 6.887199087519552, Learning Rate: 0.019506702687208693\n",
      "Epoch 225/10000, Erreurs: 18, T+: 695.6766755070256, T-: 6.956766755070255, Learning Rate: 0.01950233416435588\n",
      "Epoch 226/10000, Erreurs: 19, T+: 702.7037126333593, T-: 7.027037126333592, Learning Rate: 0.019497947126252475\n",
      "Epoch 227/10000, Erreurs: 19, T+: 709.8017299326862, T-: 7.098017299326861, Learning Rate: 0.01949354158585407\n",
      "Epoch 228/10000, Erreurs: 19, T+: 716.9714443764507, T-: 7.169714443764506, Learning Rate: 0.01948911755616882\n",
      "Epoch 229/10000, Erreurs: 19, T+: 724.213580178233, T-: 7.24213580178233, Learning Rate: 0.019484675050257364\n",
      "Epoch 230/10000, Erreurs: 20, T+: 731.528868866902, T-: 7.31528868866902, Learning Rate: 0.019480214081232763\n",
      "Epoch 231/10000, Erreurs: 18, T+: 738.9180493605071, T-: 7.389180493605071, Learning Rate: 0.019475734662260442\n",
      "Epoch 232/10000, Erreurs: 19, T+: 746.3818680409163, T-: 7.463818680409163, Learning Rate: 0.019471236806558125\n",
      "Epoch 233/10000, Erreurs: 19, T+: 753.9210788292083, T-: 7.539210788292084, Learning Rate: 0.01946672052739577\n",
      "Epoch 234/10000, Erreurs: 19, T+: 761.5364432618267, T-: 7.615364432618266, Learning Rate: 0.019462185838095496\n",
      "Epoch 235/10000, Erreurs: 19, T+: 769.2287305675017, T-: 7.692287305675016, Learning Rate: 0.01945763275203152\n",
      "Epoch 236/10000, Erreurs: 19, T+: 776.9987177449511, T-: 7.7699871774495115, Learning Rate: 0.0194530612826301\n",
      "Epoch 237/10000, Erreurs: 20, T+: 784.8471896413648, T-: 7.848471896413648, Learning Rate: 0.019448471443369468\n",
      "Epoch 238/10000, Erreurs: 20, T+: 792.7749390316817, T-: 7.927749390316817, Learning Rate: 0.019443863247779743\n",
      "Epoch 239/10000, Erreurs: 19, T+: 800.7827666986684, T-: 8.007827666986683, Learning Rate: 0.019439236709442895\n",
      "Epoch 240/10000, Erreurs: 18, T+: 808.8714815138064, T-: 8.088714815138065, Learning Rate: 0.019434591841992657\n",
      "Epoch 241/10000, Erreurs: 19, T+: 817.0419005189964, T-: 8.170419005189965, Learning Rate: 0.019429928659114467\n",
      "Epoch 242/10000, Erreurs: 19, T+: 825.2948490090872, T-: 8.252948490090873, Learning Rate: 0.019425247174545402\n",
      "Epoch 243/10000, Erreurs: 19, T+: 833.6311606152397, T-: 8.336311606152396, Learning Rate: 0.0194205474020741\n",
      "Epoch 244/10000, Erreurs: 20, T+: 842.051677389131, T-: 8.42051677389131, Learning Rate: 0.019415829355540704\n",
      "Epoch 245/10000, Erreurs: 18, T+: 850.5572498880111, T-: 8.505572498880111, Learning Rate: 0.01941109304883679\n",
      "Epoch 246/10000, Erreurs: 18, T+: 859.1487372606173, T-: 8.591487372606172, Learning Rate: 0.01940633849590529\n",
      "Epoch 247/10000, Erreurs: 19, T+: 867.8270073339569, T-: 8.678270073339569, Learning Rate: 0.019401565710740448\n",
      "Epoch 248/10000, Erreurs: 19, T+: 876.5929367009666, T-: 8.765929367009665, Learning Rate: 0.01939677470738772\n",
      "Epoch 249/10000, Erreurs: 18, T+: 885.4474108090571, T-: 8.854474108090571, Learning Rate: 0.019391965499943733\n",
      "Epoch 250/10000, Erreurs: 19, T+: 894.3913240495527, T-: 8.943913240495526, Learning Rate: 0.019387138102556198\n",
      "Epoch 251/10000, Erreurs: 19, T+: 903.4255798480331, T-: 9.034255798480329, Learning Rate: 0.01938229252942384\n",
      "Epoch 252/10000, Erreurs: 19, T+: 912.5510907555889, T-: 9.125510907555888, Learning Rate: 0.019377428794796345\n",
      "Epoch 253/10000, Erreurs: 19, T+: 921.7687785409989, T-: 9.21768778540999, Learning Rate: 0.019372546912974277\n",
      "Epoch 254/10000, Erreurs: 19, T+: 931.0795742838374, T-: 9.310795742838373, Learning Rate: 0.019367646898309002\n",
      "Epoch 255/10000, Erreurs: 19, T+: 940.4844184685226, T-: 9.404844184685224, Learning Rate: 0.01936272876520264\n",
      "Epoch 256/10000, Erreurs: 19, T+: 949.9842610793157, T-: 9.499842610793156, Learning Rate: 0.019357792528107976\n",
      "Epoch 257/10000, Erreurs: 19, T+: 959.5800616962786, T-: 9.595800616962784, Learning Rate: 0.019352838201528385\n",
      "Epoch 258/10000, Erreurs: 19, T+: 969.2727895922005, T-: 9.692727895922003, Learning Rate: 0.01934786580001778\n",
      "Epoch 259/10000, Erreurs: 19, T+: 979.0634238305056, T-: 9.790634238305053, Learning Rate: 0.019342875338180528\n",
      "Epoch 260/10000, Erreurs: 19, T+: 988.9529533641471, T-: 9.889529533641468, Learning Rate: 0.019337866830671385\n",
      "Epoch 261/10000, Erreurs: 19, T+: 998.9423771355022, T-: 9.98942377135502, Learning Rate: 0.019332840292195414\n",
      "Epoch 262/10000, Erreurs: 19, T+: 1009.0327041772749, T-: 10.090327041772747, Learning Rate: 0.019327795737507925\n",
      "Epoch 263/10000, Erreurs: 18, T+: 1019.2249537144191, T-: 10.192249537144189, Learning Rate: 0.019322733181414396\n",
      "Epoch 264/10000, Erreurs: 19, T+: 1029.52015526709, T-: 10.295201552670898, Learning Rate: 0.0193176526387704\n",
      "Arrêt prématuré après 265 époques en raison de l'absence d'amélioration.\n",
      "Erreur d'apprentissage (Ea) : 0.1826923076923077\n",
      "Erreur de généralisation (Eg) : 0.25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_error(weights, training_features, training_labels):\n",
    "    errors = 0\n",
    "    for inputs, label in zip(training_features, training_labels):\n",
    "        inputs_with_bias = np.insert(inputs, 0, 1)\n",
    "        prediction = np.sign(np.dot(weights, inputs_with_bias))\n",
    "        if prediction != label:\n",
    "            errors += 1\n",
    "    return errors\n",
    "\n",
    "def minimerror_perceptron4(training_features, training_labels, epochs, initial_learning_rate, T_plus_initial, T_minus_initial, T_decrease_factor, patience, decay):\n",
    "    weights = np.random.uniform(low=0.3, high=0.4, size=(training_features.shape[1] + 1))\n",
    "    T_plus = T_plus_initial\n",
    "    T_minus = T_minus_initial\n",
    "    previous_errors = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    learning_rate = initial_learning_rate\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, label in zip(training_features, training_labels):\n",
    "            inputs_with_bias = np.insert(inputs, 0, 1)\n",
    "            stability = (np.dot(inputs_with_bias, weights) * label) / np.linalg.norm(weights[1:])\n",
    "            T = T_plus if stability >= 0 else T_minus\n",
    "            derivative = -inputs_with_bias * label / (4 * T * np.cosh(stability / (2 * T))**2)\n",
    "            weights[1:] -= learning_rate * derivative[1:] / np.linalg.norm(weights[1:])\n",
    "            weights[0] -= learning_rate * derivative[0]\n",
    "\n",
    "        current_errors = calculate_error(weights, training_features, training_labels)\n",
    "\n",
    "        if current_errors < previous_errors:\n",
    "            previous_errors = current_errors\n",
    "            no_improvement_count = 0\n",
    "            # Diminuez les températures lorsque les erreurs s'améliorent\n",
    "            T_plus *= T_decrease_factor\n",
    "            T_minus *= T_decrease_factor\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            # Augmentez les températures si les erreurs augmentent ou stagnent\n",
    "            T_plus /= T_decrease_factor\n",
    "            T_minus /= T_decrease_factor\n",
    "\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f'Arrêt prématuré après {epoch+1} époques en raison de l\\'absence d\\'amélioration.')\n",
    "            break\n",
    "\n",
    "        # Réduisez le taux d'apprentissage progressivement\n",
    "        learning_rate *= (1.0 / (1.0 + decay * epoch))\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Erreurs: {current_errors}, T+: {T_plus}, T-: {T_minus}, Learning Rate: {learning_rate}')\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Exemple d'utilisation de la méthode minimerror_perceptron :\n",
    "# Vous devrez définir training_features, training_labels et ajuster les hyperparamètres ci-dessous selon votre cas d'usage.\n",
    "\n",
    "# Exemple d'utilisation de la méthode minimerror_perceptron:\n",
    "weights4 = minimerror_perceptron4(train_features, train_labels, epochs=10000, initial_learning_rate=0.02, T_plus_initial=100, T_minus_initial=1, T_decrease_factor=0.99, patience=200, decay=1e-6)\n",
    "\n",
    "\n",
    "\n",
    "training_accuracy3 = calculate_accuracy(train_features, train_labels, weights4)\n",
    "testing_accuracy3 = calculate_accuracy(test_features, test_labels, weights4)\n",
    "\n",
    "\n",
    "Ea3 = 1 - training_accuracy3\n",
    "Eg3 = 1 - testing_accuracy3\n",
    "\n",
    "print(f\"Erreur d'apprentissage (Ea) : {Ea3}\")\n",
    "print(f\"Erreur de généralisation (Eg) : {Eg3}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
